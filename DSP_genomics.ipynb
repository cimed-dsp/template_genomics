{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSP_genomics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSobDMSJbyFc"
      },
      "source": [
        "# Genomics, Transcriptomics, Proteomics Data Science Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOXSqMGXSuKd"
      },
      "source": [
        "## Overview\r\n",
        "\r\n",
        "High-throughput technologies have fueled the growth of genomic, transcriptomic, epigenomic, and proteomic datasets related to healthy, diseased, and perturbed states of cellular tissues.  There are many important challenges and opportunities ([2020 viewpoint](https://www.nature.com/articles/s41576-020-0272-6)) that need to be addressed as the insights possible from these types of data analyses continue their integration into clinical practice. For this project, we have selected a tutorial that mimics the recent [KnowEnG publication](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000583), which applies various machine learning and graph mining techniques to the downstream analysis of data from The Cancer Genome Atlas (TCGA). The main purposes of this tutorial are to show an approach to cluster cancer patients using data from multiple experimental assays and how to use prior knowledge of gene relationships to identify the transcripts that relate to specific cancer subtypes.  The TCGA data, KnowEnG knowledge-guided analysis pipelines, and ideas presented in the tutorial may be useful in guiding potential data science projects. While reading the related papers and working through this analysis, you may want to consider the the critical questions for data analysis related to the DSP Course [Competencies](https://cimed-dsp.github.io/competency.html).\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWP50h6Vb2p3"
      },
      "source": [
        "## Tutorial Highlights\r\n",
        "\r\n",
        "- **Topics:** downstream multi-omics cancer data analysis and visualization, -omics analysis guided by prior knowledge\r\n",
        "- **Methods:** sample clustering (NBS and multi-omics COCA), gene prioritization (ProGENI), gene set characterization (DRaWR)\r\n",
        "- **Data:** transcriptomic, epigenomic, proteomic, and somatic mutation data relating to around 4000 samples from 12 different cancer types generated by the pan-cancer TCGA project\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir2TYUAxcCZo"
      },
      "source": [
        "## Tutorial Related Resources\r\n",
        "\r\n",
        "1. *Knowledge-guided analysis of \"omics\" data using the KnowEnG cloud platform* ([link to paper](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000583))\r\n",
        "2. *Multiplatform Analysis of 12 Cancer Types Reveals Molecular Classification within and across Tissues of Origin* ([link to paper](https://www.sciencedirect.com/science/article/pii/S0092867414008769?via%3Dihub))\r\n",
        "3. KnowEnG Knowledge-Guided Analysis [Platform](https://knoweng.org/analyze/) and Methods:\r\n",
        "   1. Network Based Stratification ([NBS](https://www.nature.com/articles/nmeth.2651))\r\n",
        "   2. Cluster-of-Cluster-Assignment ([COCA](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3465532/?report=reader))\r\n",
        "   3. Prioritization of Genes Enhanced with Network Information ([ProGENI](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1282-3))\r\n",
        "   4. Gene Set Characterization with Discriminative Random Walks with Restart ([DRaWR](https://academic.oup.com/bioinformatics/article/32/14/2167/1742836))\r\n",
        "4. KnowEnG Pipeline Tutorials: ([platform demo](https://github.com/KnowEnG/quickstart-demos/tree/master/publication_data/blatti_et_al_2019), [course lab](http://publish.illinois.edu/compgenomicscourse/files/2020/06/08_Clustering_and_Prioritization.pdf), [quickstart demos](https://knoweng.org/quick-start/), and [data preparation guide](https://github.com/KnowEnG/quickstart-demos/blob/master/pipeline_readmes/README-DataPrep.md))\r\n",
        "5. Genomic Data Commons with TCGA and other datasets: ([webportal](https://gdc.cancer.gov/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMIyW1vzb8KO"
      },
      "source": [
        "## KnowEnG Platform\r\n",
        "While this template project can be executed with the python code contained in this notebook, the analyses here and in the recent [KnowEnG publication](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000583) can be replicated through a cloud-based user interface. To follow this approach, sign up for a free account at the [KnowEnG Analysis Platform](https://knoweng.org/analyze/). Then, follow the instructions to recreate Run 1, 2, 4, and 5 at the KnowEnG analysis [platform tutorial](https://github.com/KnowEnG/quickstart-demos/tree/master/publication_data/blatti_et_al_2019). While not identical to the runs below, this tutorial covers similar concepts. The KnowEnG Platform also contains several visual interfaces to explore the results and a spreadsheet visualizer for viewing sample data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuZAP9V_dh3x"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZlQjjAmJuSY"
      },
      "source": [
        "## Using This Notebook\n",
        "\n",
        "This notebook is an interactive environment that combines explanatory text with executable code. It provides computational tools useful in genomics, and you will familiarize yourself with them by stepping through a series of analyses that examine related data from multiple angles. This suite of tools and example can then serve as a starting point for a project of your own design.\n",
        "\n",
        "If you are new to notebooks, you might find this introduction helpful: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) You might also want to refer to the [Python 3 documentation](https://docs.python.org/3/).\n",
        "\n",
        "**Important Note**: After a period of inactivity (Google does not specify exactly how long), Google will disconnect your notebook from the virtual machine that had been running it. When you return, Google will connect to a new virtual machine. Any data files you saved to your Google Drive will remain, but any variables or methods defined in your previous virtual machine will have to be reloaded. (You will know when this happens, because cells that previously ran without error will suddenly stop working, and the notebook will lose its connection to your Google Drive.) To reload the variable and method definitions, and to restore the connection to your Google Drive, you can simply re-run the cells that perform those tasks. This notebook will explain which cells might need to be re-run.\n",
        "\n",
        "### Before You Proceed\n",
        "\n",
        "In order to run the notebook, you will need your own copy of it, along with your own copy of the data. Here are the steps to follow:\n",
        "\n",
        "1. If you have not already enabled Google Apps @ Illinois, which allows you to use Google Drive, Google Docs, and so on with your illinois.edu account, [enable Google Apps @ Illinois](https://answers.uillinois.edu/illinois/page.php?id=55049).\n",
        "2. Check the currently active Google account on this notebook. Look near the top-right corner of the screen for either a `Sign In` button or a round icon containing either a letter or your profile photo. If you see a `Sign In` button, click it and follow the prompts to sign in with your illinois.edu account. Otherwise, click the icon to open a popup that identifies the currenly active Google account. If the account is not your illinois.edu account, switch to your illinois.edu account (you might have to click `Add Account` if it is not already an option in the list).\n",
        "3. In the `File` menu above, select `Save a copy in Drive...`. This step will create a new browser tab containing your copy of the notebook. At this point, you can close the old browser tab that contained the original copy of the notebook.\n",
        "4. Open the `File` menu and select `Locate in Drive`, which will show you where you can find the notebook if you need to open it later.\n",
        "5. [Click here](https://drive.google.com/drive/folders/1vsIBpVR0xi56u0WtihppTXrWscDBO70U?usp=sharing) to open the master copy of the example data in a new tab. In the new tab, again make sure the active Google account is your illinois.edu account. Click on the small triangle that appears after the folder name near the top of the screen. From the menu that appears, select `Add to My Drive`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RczciUOYuOzp"
      },
      "source": [
        "## KnowEnG Package Installation\n",
        "\n",
        "The cell below installs software required to perform the analyses. Run the cell and wait for it to complete, which will take about two minutes. You'll see lots of text output as the cell runs, but there's no need to read it unless the following cell fails.\n",
        "\n",
        "You **will** need to re-run this cell whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgrIxPJT-UiT"
      },
      "source": [
        "!pip3 install -I pyyaml==5.1.2\n",
        "!pip3 install xmlrunner==1.7.7 redis==3.3.8 lifelines==0.22.8\n",
        "!pip3 install git+https://github.com/KnowEnG/KnowEnG_Pipelines_Library.git@mjberry/update_dependencies\n",
        "!pip3 install git+https://github.com/KnowEnG/Data_Cleanup_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/General_Clustering_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/Samples_Clustering_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/Feature_Prioritization_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/Gene_Prioritization_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/Geneset_Characterization_Pipeline.git@mjberry/create_package\n",
        "!pip3 install git+https://github.com/KnowEnG/Spreadsheets_Transformation.git@mjberry/create_package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftgYqoTDuzOu"
      },
      "source": [
        "## Python Environment Setup\n",
        "\n",
        "The cell below sets up the environment for running the analyses. Run the cell and wait for it to complete, which will only take a few seconds. You won't see any text output this time.\n",
        "\n",
        "You probably will not need to call any of the methods defined in this method, and you probably will not need to edit anything in the cell.\n",
        "\n",
        "You **will** need to re-run the cell whenever your notebook connects to a new virtual machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ifxh2nzm173"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "from tempfile import mkdtemp\n",
        "import urllib.request\n",
        "\n",
        "from IPython.display import HTML\n",
        "from lifelines import KaplanMeierFitter\n",
        "from lifelines.statistics import multivariate_logrank_test\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from kndatacleanup import data_cleanup\n",
        "from knfeatureprioritization import feature_prioritization\n",
        "from kngeneprioritization import gene_prioritization\n",
        "from kngenesetcharacterization import geneset_characterization\n",
        "from knsamplesclustering import samples_clustering\n",
        "from kngeneralclustering import general_clustering\n",
        "from knspreadsheetstransformation.spreadsheets_transformation_toolbox import \\\n",
        "    get_cluster_binary_dataframe\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "NETWORK_DIR_PATH = '/network/'\n",
        "\n",
        "REDIS_PARAMS = {\n",
        "    'host': 'knowredis.knoweng.org',\n",
        "    'password': 'KnowEnG',\n",
        "    'port': 6379\n",
        "}\n",
        "\n",
        "NUM_CPUS = 2\n",
        "\n",
        "def fetch_network(edge_file_path):\n",
        "    \"\"\"Given the local path to an edge file, ensures that the edge file exists\n",
        "    on disk, downloading it from AWS if necessary.\n",
        "\n",
        "    Arguments:\n",
        "        edge_file_path (str): The local path to an edge file as found in the\n",
        "            data returned by `get_interaction_networks` and\n",
        "            `get_gene_property_networks`.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(edge_file_path):\n",
        "        url = \"https://s3.amazonaws.com/KnowNets/KN-20rep-1706/\" + \\\n",
        "            \"userKN-20rep-1706/\" + edge_file_path[len(NETWORK_DIR_PATH):]\n",
        "        os.makedirs(os.path.dirname(edge_file_path), exist_ok=True)\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with open(edge_file_path, 'wb') as out_file:\n",
        "                shutil.copyfileobj(response, out_file)\n",
        "\n",
        "def fetch_network_metadata():\n",
        "    \"\"\"Downloads from AWS the network overview metadata files required to\n",
        "    implement the network utility methods.\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    filenames = ['db_contents.txt', 'species_desc.txt', 'edge_type.txt']\n",
        "    for filename in filenames:\n",
        "        out_file_path = os.path.join(NETWORK_DIR_PATH, filename)\n",
        "        if not os.path.isfile(out_file_path):\n",
        "            url = \"https://s3.amazonaws.com/KnowNets/KN-20rep-1706/\" + \\\n",
        "                \"userKN-20rep-1706/\" + filename\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                with open(out_file_path, 'wb') as out_file:\n",
        "                    shutil.copyfileobj(response, out_file)\n",
        "\n",
        "def get_path_to_newest_file_having_prefix(search_dir_path, prefix):\n",
        "    \"\"\"Finds all files in `search_dir_path` whose name begins with `prefix`.\n",
        "    Returns the newest of these matching files, or returns None if no matching\n",
        "    file exists.\n",
        "\n",
        "    Arguments:\n",
        "        search_dir_path (str): The local path to the directory to search.\n",
        "        prefix (str): The string used to filter the files in `search_dir_path`.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the newest matching file, or None if no matching files\n",
        "            exist.\n",
        "\n",
        "    \"\"\"\n",
        "    matches = [os.path.join(search_dir_path, name) \\\n",
        "        for name in os.listdir(search_dir_path) \\\n",
        "        if name.startswith(prefix)]\n",
        "    # ensure they're all files\n",
        "    matches = [m for m in matches if os.path.isfile(m)]\n",
        "    return_val = None\n",
        "    if matches:\n",
        "        return_val = sorted(matches, \\\n",
        "            key=lambda path: os.path.getctime(path), reverse=True)[0]\n",
        "    return return_val\n",
        "\n",
        "def get_cleaned_file_path(original_file_path, results_dir_path):\n",
        "    \"\"\"Given the name of a file passed to `kndatacleanup.data_cleanup`,\n",
        "    along with the `results_dir_path` passed to `kndatacleanup.data_cleanup`,\n",
        "    returns the path at which the cleaned version of the input can be found.\n",
        "\n",
        "    Arguments:\n",
        "        original_file_path (str): The path to the file that was passed to\n",
        "            `kndatacleanup.data_cleanup`.\n",
        "        results_dir_path (str): The path to the results directory that was\n",
        "            passed to `kndatacleanup.data_cleanup`.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the cleaned version of `original_file_path` that was\n",
        "            or would be produced by `kndatacleanup.data_cleanup`.\n",
        "\n",
        "    \"\"\"\n",
        "    original_name = os.path.basename(original_file_path)\n",
        "    original_name_root = os.path.splitext(original_name)[0]\n",
        "    return os.path.join(results_dir_path, original_name_root + \"_ETL.tsv\")\n",
        "\n",
        "def get_gene_map_file_path(original_file_path, results_dir_path):\n",
        "    \"\"\"Given the name of an omics file passed to `kndatacleanup.data_cleanup`,\n",
        "    along with the `results_dir_path` passed to `kndatacleanup.data_cleanup`,\n",
        "    returns the path at which the mapping of gene names to gene identifiers\n",
        "    can be found.\n",
        "\n",
        "    Arguments:\n",
        "        original_file_path (str): The path to the omics file that was passed to\n",
        "            `kndatacleanup.data_cleanup`.\n",
        "        results_dir_path (str): The path to the results directory that was\n",
        "            passed to `kndatacleanup.data_cleanup`.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the gene-name mapping file that was or would be\n",
        "            produced by `kndatacleanup.data_cleanup`.\n",
        "\n",
        "    \"\"\"\n",
        "    original_name = os.path.basename(original_file_path)\n",
        "    original_name_root = os.path.splitext(original_name)[0]\n",
        "    return os.path.join(results_dir_path, original_name_root + \"_MAP.tsv\")\n",
        "\n",
        "os.makedirs(NETWORK_DIR_PATH, exist_ok=True)\n",
        "fetch_network_metadata()\n",
        "\n",
        "!rm -rf /content/sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4DCoKLVwBvT"
      },
      "source": [
        "## Knowledge Network Utility Methods\n",
        "\n",
        "The cell below defines several utility methods for working with the knowledge network. These methods are used in the example analyses and might be useful to you in your project. Run the cell and wait for it to complete, which will only take a second or so. It won't produce any text output.\n",
        "\n",
        "You probably will not need to edit anything within the cell.\n",
        "\n",
        "A later cell shows how to use the knowledge network utility methods.\n",
        "\n",
        "You **will** need to re-run the cell whenever your notebook connects to a new virtual machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKn3Uxx6rLce"
      },
      "source": [
        "def get_network_species():\n",
        "    \"\"\"Returns information about the species found in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        list: A list in which each element is a dictionary. Each dictionary has\n",
        "            keys 'id' (which can be passed to other methods that require a\n",
        "            `species_id`), 'short_latin_name', 'latin_name', 'familiar_name',\n",
        "            and 'group_name'.\n",
        "\n",
        "    \"\"\"\n",
        "    return_val = []\n",
        "    species_file_path = os.path.join(NETWORK_DIR_PATH, 'species_desc.txt')\n",
        "    with open(species_file_path) as csvfile:\n",
        "        for row in csv.reader(csvfile, delimiter='\\t'):\n",
        "            return_val.append({\n",
        "                'id': row[0],\n",
        "                'short_latin_name': row[1],\n",
        "                'latin_name': row[2],\n",
        "                'familiar_name': row[3],\n",
        "                'group_name': row[5]\n",
        "            })\n",
        "    return return_val\n",
        "\n",
        "def display_network_species():\n",
        "    \"\"\"Displays a table of the species found in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    html_string = \"<table><tr><th>Familiar Name (Latin Name)</th><th>Species Id</th></tr>\"\n",
        "    for species in get_network_species():\n",
        "        html_string += \"<tr><td>\" + species['familiar_name'] + \" (\" + \\\n",
        "            species['latin_name'] + \")</td><td>\" + species['id'] + \"</td></tr>\"\n",
        "    html_string += \"</table>\"\n",
        "    return HTML(html_string)\n",
        "\n",
        "def get_edge_type_name_to_pretty_name():\n",
        "    \"\"\"Returns a dictionary in which the keys are edge type names and the values\n",
        "    are pretty network names.\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary in which the keys are edge type names and the values\n",
        "            are pretty network names.\n",
        "\n",
        "    \"\"\"\n",
        "    return_val = {}\n",
        "    file_path = os.path.join(NETWORK_DIR_PATH, 'edge_type.txt')\n",
        "    with open(file_path) as csvfile:\n",
        "        for row in csv.DictReader(csvfile, delimiter='\\t'):\n",
        "            return_val[row['et_name']] = row['pretty_name']\n",
        "    return return_val\n",
        "\n",
        "def get_interaction_networks(species_id):\n",
        "    \"\"\"Given a `species_id`, returns information about the interaction networks\n",
        "    available in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`.\n",
        "\n",
        "    Returns:\n",
        "        list: A list in which each element is a dictionary. Each dictionary has\n",
        "            two keys, 'name' and 'edge_file_path'.\n",
        "\n",
        "    \"\"\"\n",
        "    species_id = str(species_id) # user-friendliness\n",
        "    return_val = []\n",
        "    contents_file_path = os.path.join(NETWORK_DIR_PATH, 'db_contents.txt')\n",
        "    with open(contents_file_path) as csvfile:\n",
        "        edge_type_name_to_pretty_name = get_edge_type_name_to_pretty_name()\n",
        "        for row in csv.DictReader(csvfile, delimiter='\\t'):\n",
        "            if row['n1_type'] == 'Gene' and row['taxon'] == species_id:\n",
        "                return_val.append({\n",
        "                    'name': edge_type_name_to_pretty_name[row['et_name']],\n",
        "                    'edge_file_path': os.path.join(\\\n",
        "                        NETWORK_DIR_PATH, 'Gene', species_id, row['et_name'], \\\n",
        "                        species_id + '.' + row['et_name'] + '.edge')\n",
        "                })\n",
        "    return return_val\n",
        "\n",
        "def display_interaction_networks(species_id):\n",
        "    \"\"\"Given a `species_id`, displays information about the interaction\n",
        "    networks available in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    html_string = \"<table><tr><th>Interaction Network Name</th><th>Edge File Path</th></tr>\"\n",
        "    for network in get_interaction_networks(species_id):\n",
        "        html_string += \"<tr><td>\" + network['name'] + \"</td><td>\" + \\\n",
        "            network['edge_file_path'] + \"</td></tr>\"\n",
        "    html_string += \"</table>\"\n",
        "    return HTML(html_string)\n",
        "\n",
        "def get_gene_property_networks(species_id):\n",
        "    \"\"\"Given a `species_id`, returns information about the gene-property\n",
        "    networks available in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`.\n",
        "\n",
        "    Returns:\n",
        "        list: A list in which each element is a dictionary. Each dictionary has\n",
        "            two keys, 'name' and 'edge_file_path'.\n",
        "\n",
        "    \"\"\"\n",
        "    species_id = str(species_id) # user-friendliness\n",
        "    return_val = []\n",
        "    contents_file_path = os.path.join(NETWORK_DIR_PATH, 'db_contents.txt')\n",
        "    with open(contents_file_path) as csvfile:\n",
        "        edge_type_name_to_pretty_name = get_edge_type_name_to_pretty_name()\n",
        "        for row in csv.DictReader(csvfile, delimiter='\\t'):\n",
        "            if row['n1_type'] == 'Property' and row['taxon'] == species_id:\n",
        "                return_val.append({\n",
        "                    'name': edge_type_name_to_pretty_name[row['et_name']],\n",
        "                    'edge_file_path': os.path.join(\\\n",
        "                        NETWORK_DIR_PATH, 'Property', species_id, row['et_name'], \\\n",
        "                        species_id + '.' + row['et_name'] + '.edge')\n",
        "                })\n",
        "    return return_val\n",
        "\n",
        "def display_gene_property_networks(species_id):\n",
        "    \"\"\"Given a `species_id`, displays information about the gene-property\n",
        "    networks available in the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    html_string = \"<table><tr><th>Interaction Network Name</th><th>Edge File Path</th></tr>\"\n",
        "    for network in get_gene_property_networks(species_id):\n",
        "        html_string += \"<tr><td>\" + network['name'] + \"</td><td>\" + \\\n",
        "            network['edge_file_path'] + \"</td></tr>\"\n",
        "    html_string += \"</table>\"\n",
        "    return HTML(html_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVvYGbnRxE-9"
      },
      "source": [
        "### Using the Knowledge Network Utility Methods\n",
        "\n",
        "The three cells below show how `display_network_species`, `display_interaction_networks`, and `display_gene_property_networks` can be called to view information about the knowledge network. This information can be useful in configuring analyses, as you'll see later.\n",
        "\n",
        "These methods are based on three other methods defined in the cell above, `get_network_species`, `get_interaction_networks`, and `get_gene_property_networks`. The \"get\" versions return the same information as the \"display\" versions, but the \"get\" versions return it in a format convenient for use in code instead of a format that's easy to read.\n",
        "\n",
        "You **will not** need to re-run these three cells whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSMHjVi6tnNc"
      },
      "source": [
        "# display all species and their taxon id in the knoweng knowledge network\n",
        "display_network_species()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uep6yYfe83ly"
      },
      "source": [
        "# display interaction networks available for humans (species id 9606)\n",
        "display_interaction_networks('9606')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNl8XB-T84x5",
        "collapsed": true
      },
      "source": [
        "#@title\n",
        "# display gene property networks for humans (species id 9606)\n",
        "display_gene_property_networks('9606')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTUV9Lp_rK0"
      },
      "source": [
        "## Analytic Pipeline Methods\n",
        "\n",
        "The cell below defines methods for running clustering, prioritization, and gene-set characterization in this notebook. Run the cell and wait for it to complete, which will take a second or so. It won't produce any output.\n",
        "\n",
        "You **will** need to re-run the cell whenever your notebook connects to a new virtual machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M6MGR3K_pd3"
      },
      "source": [
        "def do_clustering(\\\n",
        "    omics_file_path, phenotype_file_path, results_dir_path, num_clusters, \\\n",
        "    species_id, interaction_network_edge_file_path, network_influence, \\\n",
        "    num_bootstraps, bootstrap_sample_fraction):\n",
        "    \"\"\"Performs a clustering upon the samples found in `omics_file_path`.\n",
        "\n",
        "    Arguments:\n",
        "        omics_file_path (str): The path to the omics file.\n",
        "        phenotype_file_path (str): The path to a file containing phenotype data\n",
        "            on the same samples as found in `omics_file_path`, or None if no\n",
        "            phenotype data are to be analyzed. If analyzed, each phenotype will\n",
        "            scored for statistically significant differences between the\n",
        "            clusters.\n",
        "        results_dir_path (str): The path to a directory where results files\n",
        "            should be stored.\n",
        "        num_clusters (int): The number of clusters to create.\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`,\n",
        "            or None not using an `interaction_network_edge_file_path`.\n",
        "        interaction_network_edge_file_path (str): The path to an interaction\n",
        "            network edge file, to use a knowledge-guided approach to clustering,\n",
        "            or else None.\n",
        "        network_influence (float): A number between 0 and 1 that specifies the\n",
        "            amount to which network data should influence the results, or None\n",
        "            if not using an `interaction_network_edge_file_path`.\n",
        "        num_bootstraps (int): A number of bootstrap iterations to run. Use 0 for\n",
        "            no bootstrapping.\n",
        "        bootstrap_sample_fraction (float): A number between 0 and 1 that\n",
        "            specifies what fraction of the data should be used in each bootstrap\n",
        "            iteration, or None if not using bootstrapping.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        species_id = str(species_id) # user-friendliness\n",
        "        os.makedirs(results_dir_path, exist_ok=True)\n",
        "\n",
        "        if interaction_network_edge_file_path is None:\n",
        "            pipeline_type = 'general_clustering_pipeline'\n",
        "        else:\n",
        "            fetch_network(interaction_network_edge_file_path)\n",
        "            pipeline_type = 'samples_clustering_pipeline'\n",
        "\n",
        "        cleanup_parameters = {\n",
        "            'spreadsheet_name_full_path': omics_file_path,\n",
        "            'pipeline_type': pipeline_type,\n",
        "        'results_directory': results_dir_path\n",
        "        }\n",
        "        if phenotype_file_path is not None:\n",
        "            cleanup_parameters['phenotype_name_full_path'] = phenotype_file_path\n",
        "        if interaction_network_edge_file_path is not None:\n",
        "            cleanup_parameters.update({\n",
        "                'gg_network_name_full_path': interaction_network_edge_file_path,\n",
        "                'taxonid': species_id,\n",
        "                'source_hint': '',\n",
        "                'redis_credential': {\n",
        "                    'host': REDIS_PARAMS['host'],\n",
        "                    'port': REDIS_PARAMS['port'],\n",
        "                    'password': REDIS_PARAMS['password']\n",
        "                }\n",
        "            })\n",
        "        data_cleanup.run_pipelines(cleanup_parameters, \\\n",
        "            data_cleanup.SELECT[pipeline_type])\n",
        "\n",
        "        clustering_parameters = {\n",
        "            'spreadsheet_name_full_path': get_cleaned_file_path(\\\n",
        "                omics_file_path, results_dir_path),\n",
        "            'results_directory': results_dir_path,\n",
        "            'processing_method': 'parallel',\n",
        "            'parallelism': NUM_CPUS,\n",
        "            'number_of_clusters': num_clusters,\n",
        "            'run_directory': results_dir_path,\n",
        "            'tmp_directory': './tmp'\n",
        "        }\n",
        "        if phenotype_file_path is not None:\n",
        "            clustering_parameters.update({\n",
        "                'phenotype_name_full_path': get_cleaned_file_path(\\\n",
        "                    phenotype_file_path, results_dir_path),\n",
        "                'threshold': 15\n",
        "            })\n",
        "\n",
        "        method_prefix = ''\n",
        "        if num_bootstraps > 0:\n",
        "            clustering_parameters.update({\n",
        "                'number_of_bootstraps': num_bootstraps,\n",
        "                'rows_sampling_fraction': 1.0,\n",
        "                'cols_sampling_fraction': bootstrap_sample_fraction\n",
        "            })\n",
        "            method_prefix = 'cc_'\n",
        "\n",
        "        if interaction_network_edge_file_path is not None:\n",
        "            clustering_parameters.update({\n",
        "                'gg_network_name_full_path': interaction_network_edge_file_path,\n",
        "                'rwr_max_iterations': 100,\n",
        "                'rwr_convergence_tolerence': 1.0e-4,\n",
        "                'rwr_restart_probability': network_influence,\n",
        "                'top_number_of_genes': 100,\n",
        "                'nmf_conv_check_freq': 50,\n",
        "                'nmf_max_invariance': 200,\n",
        "                'nmf_max_iterations': 10000,\n",
        "                'nmf_penalty_parameter': 1400,\n",
        "                'method': method_prefix + 'net_nmf'\n",
        "            })\n",
        "            samples_clustering.SELECT[clustering_parameters['method']](\\\n",
        "                clustering_parameters)\n",
        "        else:\n",
        "            clustering_parameters.update({\n",
        "                'top_number_of_rows': 100,\n",
        "                'affinity_metric': 'euclidean',\n",
        "                'linkage_criterion': 'ward',\n",
        "                'method': method_prefix + 'hclust'\n",
        "            })\n",
        "            general_clustering.SELECT[clustering_parameters['method']](\\\n",
        "                clustering_parameters)\n",
        "    except:\n",
        "        print(\"Something went wrong! Check the debugging information below, \" + \\\n",
        "            \"and look for log output in \" + results_dir_path)\n",
        "        raise\n",
        "    else:\n",
        "        print(\"Find results in \" + results_dir_path)\n",
        "\n",
        "def do_prioritization(\\\n",
        "    omics_file_path, phenotype_file_path, results_dir_path, \\\n",
        "    correlation_measure, missing_value_strategy, num_exported_features, \\\n",
        "    num_response_correlated_features, species_id, \\\n",
        "    interaction_network_edge_file_path, network_influence):\n",
        "    \"\"\"Prioritizes the features (genes or otherwise) found in `omics_file_path`\n",
        "    for each phenotype found in `phenotype_file_path`.\n",
        "\n",
        "    Arguments:\n",
        "        omics_file_path (str): The path to the omics file.\n",
        "        phenotype_file_path (str): The path to a file containing phenotype data\n",
        "            on the same samples as found in `omics_file_path`.\n",
        "        results_dir_path (str): The path to a directory where results files\n",
        "            should be stored.\n",
        "        correlation_measure (str): Either 't_test' for binary or categorical\n",
        "            phenotypes or 'pearson' for numeric phenotypes.\n",
        "        missing_value_strategy (str): Governs how to handle missing values in\n",
        "            `omics_file_path`. Options are 'average' to use the average value\n",
        "            for the feature among the other samples, 'remove' to drop any\n",
        "            samples with missing values, or 'reject' to fail if any missing\n",
        "            values are found (perhaps as a sanity check if you believe missing\n",
        "            values were prevented upstream).\n",
        "        num_exported_features (int): The number of top features per phenotype to\n",
        "            include in the matrix that can be passed to `do_characterization`.\n",
        "        num_response_correlated_features (int): The number of top features to\n",
        "            retain from the first stage of the analysis if using an\n",
        "            `interaction_network_edge_file_path`.\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`,\n",
        "            or none if not using an `interactive_network_edge_file_path`.\n",
        "        interaction_network_edge_file_path (str): The path to an interaction\n",
        "            network edge file, to use a knowledge-guided approach to\n",
        "            prioritization, or else None.\n",
        "        network_influence (float): A number between 0 and 1 that specifies the\n",
        "            amount to which network data should influence the results, or None\n",
        "            if not using an `interaction_network_edge_file_path`.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        species_id = str(species_id) # user-friendliness\n",
        "        os.makedirs(results_dir_path, exist_ok=True)\n",
        "\n",
        "        if interaction_network_edge_file_path is None:\n",
        "            pipeline_type = 'feature_prioritization_pipeline'\n",
        "        else:\n",
        "            fetch_network(interaction_network_edge_file_path)\n",
        "            pipeline_type = 'gene_prioritization_pipeline'\n",
        "\n",
        "        cleanup_parameters = {\n",
        "            'spreadsheet_name_full_path': omics_file_path,\n",
        "            'phenotype_name_full_path': phenotype_file_path,\n",
        "            'pipeline_type': pipeline_type,\n",
        "            'correlation_measure': correlation_measure, # t_test, pearson, edgeR\n",
        "            'impute': missing_value_strategy, # average, remove, reject\n",
        "            'results_directory': results_dir_path\n",
        "        }\n",
        "        if interaction_network_edge_file_path is not None:\n",
        "            cleanup_parameters.update({\n",
        "                'taxonid': species_id,\n",
        "                'source_hint': '',\n",
        "                'redis_credential': {\n",
        "                    'host': REDIS_PARAMS['host'],\n",
        "                    'port': REDIS_PARAMS['port'],\n",
        "                    'password': REDIS_PARAMS['password']\n",
        "                }\n",
        "            })\n",
        "        data_cleanup.run_pipelines(cleanup_parameters, \\\n",
        "            data_cleanup.SELECT[pipeline_type])\n",
        "\n",
        "        prioritization_parameters = {\n",
        "            'correlation_measure': correlation_measure,\n",
        "            'spreadsheet_name_full_path': get_cleaned_file_path(\\\n",
        "                omics_file_path, results_dir_path),\n",
        "            'phenotype_name_full_path': get_cleaned_file_path(\\\n",
        "                phenotype_file_path, results_dir_path),\n",
        "            'results_directory': results_dir_path,\n",
        "            'top_gamma_of_sort': num_exported_features,\n",
        "            'max_cpu': NUM_CPUS\n",
        "        }\n",
        "        if interaction_network_edge_file_path is not None:\n",
        "            prioritization_parameters.update({\n",
        "                'gg_network_name_full_path': interaction_network_edge_file_path,\n",
        "                'rwr_max_iterations': 100,\n",
        "                'rwr_convergence_tolerence': 1.0e-4,\n",
        "                'rwr_restart_probability': network_influence,\n",
        "                'top_beta_of_sort': num_response_correlated_features,\n",
        "                'method': 'net_correlation'\n",
        "            })\n",
        "            gene_prioritization.net_correlation(prioritization_parameters)\n",
        "        else:\n",
        "            prioritization_parameters.update({\n",
        "                'top_beta_of_sort': num_exported_features,\n",
        "                'method': 'correlation',\n",
        "            })\n",
        "            feature_prioritization.correlation(prioritization_parameters)\n",
        "    except:\n",
        "        print(\"Something went wrong! Check the debugging information below, \" + \\\n",
        "            \"and look for log output in \" + results_dir_path)\n",
        "        raise\n",
        "    else:\n",
        "        print(\"Find results in \" + results_dir_path)\n",
        "\n",
        "def do_characterization(\\\n",
        "    gene_matrix_file_path, results_dir_path, species_id, \\\n",
        "    gene_property_edge_file_path, interaction_network_edge_file_path, \\\n",
        "    network_influence):\n",
        "    \"\"\"Compares user-submitted gene sets to those found in a gene-property\n",
        "    network from the knowledge network.\n",
        "\n",
        "    Arguments:\n",
        "        gene_matrix_file_path (str): The path to the gene matrix that defines\n",
        "            one or more gene sets.\n",
        "        results_dir_path (str): The path to a directory where results files\n",
        "            should be stored.\n",
        "        species_id (int or str): The id for the species of interest, as returned\n",
        "            by `get_network_species` or displayed by `display_network_species`.\n",
        "        gene_property_edge_file_path: The path to a gene-property network edge\n",
        "            file.\n",
        "        interaction_network_edge_file_path (str): The path to an interaction\n",
        "            network edge file, to use a knowledge-guided approach to\n",
        "            characterization, or else None.\n",
        "        network_influence (float): A number between 0 and 1 that specifies the\n",
        "            amount to which network data should influence the results, or None\n",
        "            if not using an `interaction_network_edge_file_path`.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        species_id = str(species_id) # user-friendliness\n",
        "        os.makedirs(results_dir_path, exist_ok=True)\n",
        "\n",
        "        fetch_network(gene_property_edge_file_path)\n",
        "\n",
        "        cleanup_parameters = {\n",
        "            'spreadsheet_name_full_path': gene_matrix_file_path,\n",
        "            'pipeline_type': 'geneset_characterization_pipeline',\n",
        "            'results_directory': results_dir_path,\n",
        "            'taxonid': species_id,\n",
        "            'source_hint': '',\n",
        "            'redis_credential': {\n",
        "                'host': REDIS_PARAMS['host'],\n",
        "                'port': REDIS_PARAMS['port'],\n",
        "                'password': REDIS_PARAMS['password']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        data_cleanup.run_pipelines(cleanup_parameters, \\\n",
        "            data_cleanup.SELECT['geneset_characterization_pipeline'])\n",
        "\n",
        "        characterization_parameters = {\n",
        "            'spreadsheet_name_full_path': get_cleaned_file_path(\\\n",
        "                gene_matrix_file_path, results_dir_path),\n",
        "            'gene_names_map': get_gene_map_file_path(\\\n",
        "                gene_matrix_file_path, results_dir_path),\n",
        "            'results_directory': results_dir_path,\n",
        "            'pg_network_name_full_path': gene_property_edge_file_path,\n",
        "            'max_cpu': NUM_CPUS\n",
        "        }\n",
        "        if interaction_network_edge_file_path is None:\n",
        "            characterization_parameters.update({\n",
        "                'method': 'fisher'\n",
        "            })\n",
        "            geneset_characterization.fisher(characterization_parameters)\n",
        "        else:\n",
        "            fetch_network(interaction_network_edge_file_path)\n",
        "            characterization_parameters.update({\n",
        "                'method': 'DRaWR',\n",
        "                'rwr_max_iterations': 500,\n",
        "                'rwr_convergence_tolerence': 1.0e-4,\n",
        "                'rwr_restart_probability': network_influence,\n",
        "                'gg_network_name_full_path': interaction_network_edge_file_path\n",
        "            })\n",
        "            geneset_characterization.DRaWR(characterization_parameters)\n",
        "    except:\n",
        "        print(\"Something went wrong! Check the debugging information below, \" + \\\n",
        "            \"and look for log output in \" + results_dir_path)\n",
        "        raise\n",
        "    else:\n",
        "        print(\"Find results in \" + results_dir_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRxFpjpTM0b8"
      },
      "source": [
        "## Connect to Google Drive\n",
        "\n",
        "The cell below enables this notebook to use your Google Drive for file storage. Subsequent cells will use this access to load the example files you copied earlier (step 5 of 'Using This Notebook') and to save results of the example analyses. You might also find this helpful in running your own analyses.\n",
        "\n",
        "Run the cell and click on the link that appears in the output. On the linked page, select your illinois.edu account and grant the requested permissions. The page will then display a code. Copy the code and paste it in the box that appears in the output below. Then press Enter.\n",
        "\n",
        "You **will** need to re-run the cell whenever your notebook connects to a new virtual machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTqz9ZNXvlG_"
      },
      "source": [
        "from google.colab import drive\n",
        "GDRIVE_MOUNT_PATH = '/content/gdrive'\n",
        "drive.mount(GDRIVE_MOUNT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC9dFiPKnLmv"
      },
      "source": [
        "## Setting File Locations\n",
        "\n",
        "In the cell below, we will tell the notebook where the example files can be found and where the results should be saved.\n",
        "\n",
        "To confirm the locations, find the folder symbol near the top left corner of the portion of your screen that shows the notebook content. It should appear below three other icons labeled `Table of contents`, `Find and replace` and `Code snippets`. Click on the fourth `Files` icon.\n",
        "\n",
        "In the `Files` tab, you should see one folder named `gdrive`. Click the arrow next to the `gdrive` folder to expand it, and continue navigating through the folders until you find the `Genomics Data Science Project - example analyses inputs` folder copied previously. Right-click on `Genomics Data Science Project - example analyses inputs` and select `Copy path`. Paste the value into the cell below, and compare it to the value assigned to `INPUT_DATA_DIR_PATH`. If the values are different, replace the pre-coded value with the one you pasted.\n",
        "\n",
        "This notebook is configured to store results in a folder named `Genomics Data Science Project - example analyses outputs` alongside the folder of input data. The folder will be created if it does not already exist. If you would rather store the results elsewhere, you can change the value of `OUTPUT_DATA_DIR_PATH` below.  \n",
        "\n",
        "Once you have made any changes to `INPUT_DATA_DIR_PATH` and `OUTPUT_DATA_DIR_PATH`, run the cell.\n",
        "\n",
        "If at any point you open the `Files` tab or click its `REFRESH` button and do not see `gdrive`, you might need to re-run the previous cell.\n",
        "\n",
        "Note this cell also specifies the output directories that will be used for the different analyses in the example. They are defined here, in the last quick-running cell before the analyses below, because the variables will need to be refreshed if your notebook connects to a new virtual machine.\n",
        "\n",
        "You **will** need to re-run the cell whenever your notebook connects to a new virtual machine, but the values assigned to `INPUT_DATA_DIR_PATH` and `OUTPUT_DATA_DIR_PATH` will not change unless you move the folders within your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnBOrd8rMm2A"
      },
      "source": [
        "INPUT_DATA_DIR_PATH = '/content/gdrive/My Drive/Genomics Data Science Project - example analyses inputs'\n",
        "OUTPUT_DATA_DIR_PATH = os.path.join(\\\n",
        "    os.path.dirname(INPUT_DATA_DIR_PATH),\n",
        "    'Genomics Data Science Project - example analyses outputs')\n",
        "os.makedirs(OUTPUT_DATA_DIR_PATH, exist_ok=True)\n",
        "\n",
        "CLUSTERING1_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering1')\n",
        "CLUSTERING2_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering2')\n",
        "CLUSTERING3_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering3')\n",
        "CLUSTERING4_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering4')\n",
        "CLUSTERING5_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering5')\n",
        "CLUSTERING6_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'clustering6')\n",
        "\n",
        "PRIORITIZATION1_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'prioritization1')\n",
        "PRIORITIZATION2_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'prioritization2')\n",
        "PRIORITIZATION3_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'prioritization3')\n",
        "\n",
        "CHARACTERIZATION1_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'characterization1')\n",
        "CHARACTERIZATION2_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'characterization2')\n",
        "CHARACTERIZATION3_DIR_PATH = os.path.join(OUTPUT_DATA_DIR_PATH, 'characterization3')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWsE083wAk-2"
      },
      "source": [
        "## Loading Pre-Computed Results (optional)\n",
        "\n",
        "The following sections will guide you through a series of analyses. Some of the steps are computationally intensive and require more than a few minutes to run. To avoid waiting, you may load pre-computed results into your `OUTPUT_DATA_DIR_PATH` at this point, by running the cell below and waiting until it finishes, which will take about 7 minutes. You will then be able to inspect the data, and you will still be able to run any of the remaining cells if you wish.\n",
        "\n",
        "You **will not** need to re-run this cell whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsPsLnlCDJk"
      },
      "source": [
        "src_top_dir = os.path.join(INPUT_DATA_DIR_PATH, 'precomputed results')\n",
        "dst_top_dir = OUTPUT_DATA_DIR_PATH\n",
        "\n",
        "# shutil.copytree's dirs_exist_ok not introduced until py3.8, but colab uses 3.6\n",
        "for root, dirs, files in os.walk(src_top_dir):\n",
        "    for src_dir in dirs:\n",
        "        src_dir_path = os.path.join(root, src_dir)\n",
        "        dst_dir_path = src_dir_path.replace(src_top_dir, dst_top_dir, 1)\n",
        "        os.makedirs(dst_dir_path, exist_ok=True)\n",
        "    for src_file in files:\n",
        "        src_file_path = os.path.join(root, src_file)\n",
        "        dst_file_path = src_file_path.replace(src_top_dir, dst_top_dir, 1)\n",
        "        shutil.copy2(src_file_path, dst_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXuvmFKgf0ou"
      },
      "source": [
        "# Running KnowEnG Analysis Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMjTAG0hgPeG"
      },
      "source": [
        "## Clustering\r\n",
        "\r\n",
        "In this section we will attempt to cluster multi-omic data from TCGA patient samples into meaningful cancer subtypes. There are three types of analyses for this section, each with their own set of input files.\r\n",
        "\r\n",
        "#### Standard Clustering Input Files\r\n",
        "\r\n",
        "**File Name**|**File Details**\r\n",
        "---|---|\r\n",
        "**in_clustering_clinical_data.tsv** | **Description**: clinical data for TCGA samples\r\n",
        " | **File Type**: phenotype file\r\n",
        " | **Dimensions**: 3276 samples by 28 phenotypic categories\r\n",
        " | **Original Source**: TCGA_PANCAN12_mutation-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "**in_clustering1_genecopynumber.tsv** | **Description**: TCGA PANCAN AWG gene level copy number profile, across 12 TCGA cohorts\r\n",
        " | **File Type**: omics feature file\r\n",
        " | **Dimensions**: 19723 genes by 4934 samples\r\n",
        " | **Original Source**: TCGA_PANCAN12_genecopynumber-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "**in_clustering2_exp_HiSeqV2.tsv** | **Description**: TCGA PANCAN AWG gene expression by RNAseq (IlluminaHiSeq, IlluminaGA), across 12 TCGA cohorts\r\n",
        " | **File Type**: omics feature file\r\n",
        " | **Dimensions**: 16115 genes by 3599 samples\r\n",
        " | **Original Source**: TCGA_PANCAN12_exp_HiSeqV2-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "**in_clustering3_hMethyl.tsv** | **Description**: TCGA PANCAN AWG dichotomized DNA methylation profile, across 12 TCGA cohorts\r\n",
        " | **File Type**: omics feature file\r\n",
        " | **Dimensions**: 2043 loci by 4919 samples\r\n",
        " | **Original Source**: TCGA_PANCAN12_hMethyl-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "**in_clustering4_RPPA_RBN.tsv** | **Description**: TCGA PANCAN AWG phospho- or total- protein expression by reverse phase protein array\r\n",
        " | **File Type**: omics feature file\r\n",
        " | **Dimensions**: 131 proteins by 3467 samples\r\n",
        " | **Original Source**: TCGA_PANCAN12_RPPA_RBN-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "\r\n",
        "#### Network-Based Clustering Input Files\r\n",
        "\r\n",
        "**File Name**|**File Details**\r\n",
        "---|---|\r\n",
        "**in_clustering5_mutation.tsv** | **Description**: presence of non-silent somatic mutation in protein coding region of gene for sample, gene names have been mapped to Ensembl IDs by [KN_Mapper](https://github.com/KnowEnG/KN_Mapper)\r\n",
        " | **File Type**: 0/1-valued omics feature file\r\n",
        " | **Dimensions**: 31152 genes by 3276 samples\r\n",
        " | **Original Source**: TCGA_PANCAN12_mutation-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\r\n",
        "\r\n",
        "#### COCA Analysis Input Files\r\n",
        "\r\n",
        "**File Name**|**File Details**\r\n",
        "---|---|\r\n",
        "**in_clustering6_miRNA.tsv** | **Description**: assignment of TCGA samples to subtype based on clustering miRNA data\r\n",
        " | **File Type**: cluster phenotype file\r\n",
        " | **Dimensions**: 4197 samples with 15 clusters \r\n",
        " | **Original Source**: Supplemental files from [TCGA PANCAN paper](https://www.sciencedirect.com/science/article/pii/S0092867414008769?via%3Dihub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t-CbUY6sP2K"
      },
      "source": [
        "### Standard Clustering\n",
        "\n",
        "The following four cells will use standard hierarchical clustering techniques (with 'euclidean' affinity_metric and 'ward' linkage) to group samples according to different omics data inputs. The number of clusters for each analysis match the optimal number found in the TCGA paper. Run each cell; note each cell contains a comment with an estimated running time. To better understand what each cell is doing, you can review the command in the 'Analytic Pipeline Methods' section. \n",
        "\n",
        "```\n",
        "do_clustering(\\\n",
        "    omics_file_path, phenotype_file_path, results_dir_path, num_clusters, \\\n",
        "    species_id, interaction_network_edge_file_path, network_influence, \\\n",
        "    num_bootstraps, bootstrap_sample_fraction)\n",
        "``` \n",
        " \n",
        "As each of these cells finishes, it will store the results to your Google Drive. For that reason, you **will not** need to re-run these cells whenever your notebook connects to a new virtual machine. Note, many of the generated output files will contain the current date (you may have loaded pre-computed results from an older date in the optional step above).\n",
        "\n",
        "A general description of the output files can be found at the [clustering README](https://github.com/KnowEnG/quickstart-demos/blob/master/pipeline_readmes/README-SC.md). We will be using the `sample_label_by_cluster_...` files later in the COCA analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub32Jwwksg31"
      },
      "source": [
        "# standard sample clustering of genecopynumber data takes about 9 minutes\n",
        "do_clustering(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering1_genecopynumber.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING1_DIR_PATH, 8, None, None, None, 0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKX0UCvmskVz"
      },
      "source": [
        "# standard sample clustering of gene_expression data takes about 3 minutes\n",
        "do_clustering(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering2_exp_HiSeqV2.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING2_DIR_PATH, 13, None, None, None, 0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV7S9H7FsngW"
      },
      "source": [
        "# standard sample clustering of methylation data takes less than 1 minute\n",
        "do_clustering(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering3_hMethyl.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING3_DIR_PATH, 19, None, None, None, 0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFJcuLclspo5"
      },
      "source": [
        "# standard sample clustering of proteomics data takes less than 1 minute\n",
        "do_clustering(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering4_RPPA_RBN.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING4_DIR_PATH, 8, None, None, None, 0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWMbtPJZssUT"
      },
      "source": [
        "### Network-Based Clustering\n",
        "\n",
        "This fifth clustering analysis incorporates a knowledge network in order to improve results over sparse, gene level mutation data. This has been shown to be valuable and is called network based stratification ([NBS](https://www.nature.com/articles/nmeth.2651)). In this example, the network being used is the HumanNet Integrated Gene-Gene network originally found [here](http://www.functionalnet.org/humannet/about.html). The restart parameter is set at 0.5, meaning that conceptually at every step the random walker has a 50% chance to return to a mutated gene of the restart set and a 50% chance of following a network edge. As with the above clustering analyses, run the cell and wait until it completes.  To better understand what the cell is doing, you can review the command in the 'Analytic Pipeline Methods' section. \n",
        "\n",
        "```\n",
        "do_clustering(\\\n",
        "    omics_file_path, phenotype_file_path, results_dir_path, num_clusters, \\\n",
        "    species_id, interaction_network_edge_file_path, network_influence, \\\n",
        "    num_bootstraps, bootstrap_sample_fraction)\n",
        "``` \n",
        "\n",
        "As with all of the analysis cells, it will store the results to your Google Drive. For that reason, you **will not** need to re-run this cell whenever your notebook connects to a new virtual machine. Again a general description of the output files can be found at the [clustering README](https://github.com/KnowEnG/quickstart-demos/blob/master/pipeline_readmes/README-SC.md) and we will be the `sample_label_by_cluster_...` file later in the COCA analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mnPXgrTss9S"
      },
      "source": [
        "# knowledge-guided sample clustering of mutation data takes almost 2 hours\n",
        "do_clustering(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering5_mutation.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING5_DIR_PATH, 14, '9606', \\\n",
        "    '/network/Gene/9606/hn_IntNet/9606.hn_IntNet.edge', 0.5, 0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwpR_2p-bwjo"
      },
      "source": [
        "### Cluster-of-Clusters Analysis (COCA)\n",
        "\n",
        "This sixth clustering analysis operates upon the cluster assignments generated by the previous five clustering analyses, along with the results of an additional clustering based on miRNA data. The additional clustering is not part of this notebook, but its results are provided in the folder of inputs so that this cell can load them. Conceptually, COCA represents the cluster memberships from the different omics data clusterings as sample features and then performs one final sample clustering on those combined features. In this case, we employ bootstrap sampling and find the final clustering from a co-clustering matrix built from combining the clustering results from 200 samplings of 80% of the feature matrix. Again, run the cell and wait until it finishes.\n",
        "\n",
        "As with all of the analysis cells, it will store the results to your Google Drive. For that reason, you **will not** need to re-run this cell whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Kdpf2McHdS"
      },
      "source": [
        "# takes about 8 minutes\n",
        "\n",
        "# gather the outputs from the five previous clustering analyses, along with the\n",
        "# miRNA clusters\n",
        "raw_coca_inputs = [\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING1_DIR_PATH, 'samples_label_by_cluster'),\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING2_DIR_PATH, 'samples_label_by_cluster'),\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING3_DIR_PATH, 'samples_label_by_cluster'),\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING4_DIR_PATH, 'samples_label_by_cluster'),\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING5_DIR_PATH, 'samples_label_by_cluster'),\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering6_miRNA.tsv')\n",
        "]    \n",
        "\n",
        "# assemble the raw inputs into a single file formatted like an omics file\n",
        "os.makedirs(CLUSTERING6_DIR_PATH, exist_ok=True)\n",
        "coca_input_file_path = os.path.join(CLUSTERING6_DIR_PATH, 'input.tsv')\n",
        "temp_dir_path = mkdtemp()\n",
        "try:\n",
        "    for input in raw_coca_inputs:\n",
        "        shutil.copy(input, temp_dir_path)\n",
        "    coca_input_df = get_cluster_binary_dataframe(\\\n",
        "        [os.path.basename(input) for input in raw_coca_inputs], temp_dir_path).T\n",
        "    coca_input_df.to_csv(coca_input_file_path, sep='\\t')\n",
        "finally:\n",
        "    shutil.rmtree(temp_dir_path)\n",
        "\n",
        "do_clustering(\\\n",
        "    coca_input_file_path, \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    CLUSTERING6_DIR_PATH, 13, None, None, None, 200, 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icBaJd0bG5eU"
      },
      "source": [
        "Finally, we can perform a survival analysis on the identified clusters. The following code extracts an overall survival indicator and number of survival days from the TCGA patient data and plots these values in a Kaplan-Meier plot where each curve represents one of our final COCA subtypes. Note the differences in survival outcome by our different discovered subtypes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU2YPY6TG4xJ"
      },
      "source": [
        "# takes about 1 second\n",
        "\n",
        "# load the file containing the clinical data\n",
        "# survival data are found in columns _OS_IND (boolean representing event) and\n",
        "# _OS (float indicating time)\n",
        "phenotype_df = pd.read_csv(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_clustering_clinical_data.tsv'), \\\n",
        "    sep='\\t', index_col=0, header=0)\n",
        "\n",
        "# load the cluster assignments\n",
        "cluster_labels_file_path = get_path_to_newest_file_having_prefix(\\\n",
        "    CLUSTERING6_DIR_PATH, 'samples_label_by_cluster')\n",
        "cluster_labels_df = pd.read_csv(cluster_labels_file_path, \\\n",
        "    sep='\\t', index_col=0, header=None, names=['cluster'])\n",
        "# reorder cluster_labels_df to match the sample order in phenotype_df\n",
        "\n",
        "combined_df = pd.concat([phenotype_df['_OS_IND'], phenotype_df['_OS'], \\\n",
        "        cluster_labels_df], axis=1, sort=True)\n",
        "\n",
        "# retain only the samples that have a time and a cluster\n",
        "combined_df.dropna(subset=['_OS', 'cluster'], inplace=True)\n",
        "\n",
        "# fill missing values in event (0, for censored)\n",
        "combined_df['_OS_IND'].fillna(value=0, inplace=True)\n",
        "\n",
        "# calculate p-value\n",
        "test_stats = multivariate_logrank_test(combined_df['_OS'].values, \\\n",
        "    combined_df['cluster'].values, combined_df['_OS_IND'].values)\n",
        "\n",
        "# draw plot\n",
        "fig = plt.figure()\n",
        "ax = fig.gca()\n",
        "\n",
        "kmf = KaplanMeierFitter()\n",
        "\n",
        "for name, grouped_df in combined_df.groupby('cluster'):\n",
        "    kmf.fit(grouped_df[\"_OS\"], grouped_df[\"_OS_IND\"], \\\n",
        "        label='Cluster ' + str(int(name)))\n",
        "    kmf.plot(ax=ax, show_censors=True)\n",
        "\n",
        "plt.title ('P-value = %s' %(test_stats.p_value))\n",
        "plt.xlabel('Time (days)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mRscDmTd9UF"
      },
      "source": [
        "## Gene Prioritization\n",
        "\n",
        "In this section we will find sets of genes that relate to specific cancers types using gene expression data. We will use a simple t-test to find genes that differentially expressed between the samples of each cancer type vs all others.  We will also use a knowledge-guide version of this gene prioritization called [ProGENI](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1282-3) to allow differential expression in gene neighbors to influence the results. There are two new input files for this section.\n",
        "\n",
        "#### Gene Prioritization Input Files\n",
        "\n",
        "**File Name**|**File Details**\n",
        "---|---|\n",
        "**in_prioritization_expr.tsv** | **Description**: gene normalized log2 expression of gene for sample, missing values are filled with gene mean, gene names have been mapped to Ensembl IDs by [KN_Mapper](https://github.com/KnowEnG/KN_Mapper)\n",
        " | **File Type**: omics feature file\n",
        " | **Dimensions**: 14373 genes by 3599 samples\n",
        " | **Original Source**: TCGA_PANCAN12_exp_HiSeqV2-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection, more information [here](https://www.synapse.org/#!Synapse:syn1715755)\n",
        "**pancan_disease_types.gXc** | **Description**: clinical data for TCGA samples where \"_primary_disease\" field is transformed into membership matrix by \"Category to Binary\" tool in KnowEnG Spreadsheet Transformations\n",
        " | **File Type**: 0/1-valued response file\n",
        " | **Dimensions**: 5040 samples by 12 disease types\n",
        " | **Original Source**: TCGA_PANCAN12_exp_HiSeqV2-2015-01-28.tgz file from UCSC Cancer Genome Browser PANCAN12 collection\n",
        "\n",
        "In the first of the three prioritization runs, the phenotypes are PANCAN disease types, and the method is our standard prioritization technique.\n",
        "To better understand what the cell is doing, you can review the command in the 'Analytic Pipeline Methods' section. \n",
        "\n",
        "```\n",
        "do_prioritization(\\\n",
        "    omics_file_path, phenotype_file_path, results_dir_path, \\\n",
        "    correlation_measure, missing_value_strategy, num_exported_features, \\\n",
        "    num_response_correlated_features, species_id, \\\n",
        "    interaction_network_edge_file_path, network_influence)\n",
        "``` \n",
        "\n",
        "Run the cell and wait until it finishes. As with all of the analysis cells, it will store the results to your Google Drive. For that reason, you **will not** need to re-run this cell whenever your notebook connects to a new virtual machine.\n",
        "\n",
        "A general description of the output files can be found at the [gene prioritization README](https://github.com/KnowEnG/quickstart-demos/blob/master/pipeline_readmes/README-FP.md). The main result files are the t-test scores for each cancer type and the merged `top_features_per_response_...`. Mapping of the Ensembl gene identifiers can be found [here](https://s3.amazonaws.com/KnowNets/KN-20rep-1706/userKN-20rep-1706/Species/9606/9606.node_map.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTXqLBcSfOgj"
      },
      "source": [
        "# standard prioritization for cancer type takes about 6 minutes\n",
        "do_prioritization(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_prioritization_expr.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'pancan_disease_types.gXc'), \\\n",
        "    PRIORITIZATION1_DIR_PATH, 't_test', 'average', 100, 50, None, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNkmEKAbfq7l"
      },
      "source": [
        "In the second of the three prioritization cells, the phenotypes are again the PANCAN disease types, but we now run with the ProGeNI method which incorporates the network neighborhoods from the HumanNet Integrated [network](http://www.functionalnet.org/humannet/about.html). Again, run the cell and wait until it finishes.\n",
        "\n",
        "As with all of the analysis cells, it will store the results to your Google Drive. For that reason, you **will not** need to re-run this cell whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dugvbIPxgBkq"
      },
      "source": [
        "# network-guided prioritization for cancer type takes about 14 minutes\n",
        "do_prioritization(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_prioritization_expr.tsv'), \\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'pancan_disease_types.gXc'), \\\n",
        "    PRIORITIZATION2_DIR_PATH, 't_test', 'average', 100, 50, '9606', \\\n",
        "    '/network/Gene/9606/hn_IntNet/9606.hn_IntNet.edge', 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn1YlqtogXoX"
      },
      "source": [
        "In the third of the three prioritization runs, the phenotypes are now our COCA cluster assignments rather than the clinical cancer types. Here, the standard method is also used.\n",
        "\n",
        "Run the cell and wait until it finishes.\n",
        "\n",
        "As with all of the analysis cells, it will store the results to your Google Drive. For that reason, you **will not** need to re-run this cell whenever your notebook connects to a new virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9E9efqLgnUb"
      },
      "source": [
        "# standard prioritization for COCA subtype takes about 6 minutes\n",
        "do_prioritization(\\\n",
        "    os.path.join(INPUT_DATA_DIR_PATH, 'in_prioritization_expr.tsv'), \\\n",
        "    get_path_to_newest_file_having_prefix(CLUSTERING6_DIR_PATH, 'samples_label_by_cluster'), \\\n",
        "    PRIORITIZATION3_DIR_PATH, 't_test', 'average', 100, 50, None, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWH5pdx8lVhh"
      },
      "source": [
        "## Gene-Set Characterization\n",
        "\n",
        "We now see how our Gene Prioritization gene top 100 lists related to gene sets from the Gene Ontology database and a set of known cancer drivers. We do this with the KnowEnG GeneSet Characterization pipeline. No additional inputs are needed. \n",
        "\n",
        "Each cell in this sequence corresponds to one of the three gene prioritization analyses above. Enrichment with each cancer top 100 gene set is calculated with Fisher's exact test with the functional and driver gene sets. \n",
        "\n",
        "To better understand what the cell is doing, you can review the command in the 'Analytic Pipeline Methods' section. \n",
        "\n",
        "```\n",
        "do_characterization(\\\n",
        "    gene_matrix_file_path, results_dir_path, species_id, \\\n",
        "    gene_property_edge_file_path, interaction_network_edge_file_path, \\\n",
        "    network_influence):\n",
        "```\n",
        "\n",
        "As with all of the analysis cells, these will store the results to your Google Drive. For that reason, you **will not** need to re-run these cells whenever your notebook connects to a new virtual machine.\n",
        "\n",
        "A general description of the output files can be found at the [geneset characterization README](https://github.com/KnowEnG/quickstart-demos/blob/master/pipeline_readmes/README-GSC.md). The main result files `fisher_sorted_by_property_score` (gsc_results.txt in the README) are the enrichment scores for each cancer top100 gene set and the Gene Ontology sets. Mapping for the Gene Ontology identifiers can be found  [here](https://s3.amazonaws.com/KnowNets/KN-20rep-1706/userKN-20rep-1706/Property/9606/gene_ontology/9606.gene_ontology.node_map).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqoPge_4mcHp"
      },
      "source": [
        "# enrichments with cancer type standard top 100 gene lists take less than 1 minute\n",
        "for property_edge_file_path in [\\\n",
        "        '/network/Property/9606/gene_ontology/9606.gene_ontology.edge', \\\n",
        "        '/network/Property/9606/cancer_driver_genes/9606.cancer_driver_genes.edge']:\n",
        "    do_characterization(\\\n",
        "        get_path_to_newest_file_having_prefix(PRIORITIZATION1_DIR_PATH, 'top_'), \\\n",
        "        os.path.join(CHARACTERIZATION1_DIR_PATH, os.path.basename(property_edge_file_path)), \\\n",
        "        '9606', property_edge_file_path, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEyycIpIFa9N"
      },
      "source": [
        "# enrichments with cancer type network guided top 100 gene lists take less than 1 minute\n",
        "for property_edge_file_path in [\\\n",
        "        '/network/Property/9606/gene_ontology/9606.gene_ontology.edge', \\\n",
        "        '/network/Property/9606/cancer_driver_genes/9606.cancer_driver_genes.edge']:\n",
        "    do_characterization(\\\n",
        "        get_path_to_newest_file_having_prefix(PRIORITIZATION2_DIR_PATH, 'top_'), \\\n",
        "        os.path.join(CHARACTERIZATION2_DIR_PATH, os.path.basename(property_edge_file_path)), \\\n",
        "        '9606', property_edge_file_path, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGDg9MiMFepB"
      },
      "source": [
        "# enrichments with COCA subtype standard top 100 gene lists takes less than 1 minute\n",
        "for property_edge_file_path in [\\\n",
        "        '/network/Property/9606/gene_ontology/9606.gene_ontology.edge', \\\n",
        "        '/network/Property/9606/cancer_driver_genes/9606.cancer_driver_genes.edge']:\n",
        "    do_characterization(\\\n",
        "        get_path_to_newest_file_having_prefix(PRIORITIZATION3_DIR_PATH, 'top_'), \\\n",
        "        os.path.join(CHARACTERIZATION3_DIR_PATH, os.path.basename(property_edge_file_path)), \\\n",
        "        '9606', property_edge_file_path, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47LJ3YaSbQDE"
      },
      "source": [
        "Although we only covered the standard enrichment method, KnowEnG also offers a knowledge-guided version based off of the [DRaWR](https://academic.oup.com/bioinformatics/article/32/14/2167/1742836) algorithm. This version can also be run with the `do_characterization()` command provided above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtUqn3utcAKO"
      },
      "source": [
        "# Project Extensions and Future Directions\r\n",
        "\r\n",
        "There are many possibilities to extend beyond the tutorial and create a data science project. Projects may attempt to apply these analysis techniques to explore specific hypotheses about alternative disease or tissue types. A wealth of public -omics datasets are available at several different project portals, such as [GDC](https://portal.gdc.cancer.gov/),\r\n",
        "[cBioPortal](https://www.cbioportal.org/datasets),\r\n",
        "[CCLE](https://portals.broadinstitute.org/ccle),\r\n",
        "[GEO](https://www.ncbi.nlm.nih.gov/geo/),\r\n",
        "[dbGaP](https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/collection.cgi?study_id=phs000688.v1.p1),\r\n",
        " and [GTEx](https://www.gtexportal.org/home/). Alternatively, projects may choose to investigate the effect of knowledge-guide analysis on the results, and the use of [alternative](https://knoweng.org/kn-data-references/) or [custom](https://github.com/KnowEnG/Network_Prepper) knowledge-networks.  One could modify and/or substitute the KnowEnG pipeline steps with alterative or extended statistical, machine learning, or network analysis approaches and reanalyze the same TCGA data. Projects also may be designed to build additional results visualizations and views that capture important features of the data and knowledge networks. Alternatively, teams could work to import/publish tools into genomics analysis cloud platforms (such as [Galaxy](https://usegalaxy.org/), SevenBridges Cancer Genomics Cloud [CGC](https://www.cancergenomicscloud.org/), or [Terra](https://app.terra.bio/)) and analyze cloud-based -omics datasets available there.\r\n"
      ]
    }
  ]
}